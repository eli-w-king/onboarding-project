Loaded environment variables from .env file
MCP client bridge listening on http://localhost:3002
Server started at: 2025-05-13T19:44:55.041Z
Blender MCP expected at: localhost:9876
--- /llm-agent endpoint hit ---
--- Raw OpenRouter API Response ---
{
  "id": "gen-1747165506-gBHa86AoNtj0IsRaQtwk",
  "provider": "Google",
  "model": "google/gemini-2.5-flash-preview:thinking",
  "object": "chat.completion",
  "created": 1747165506,
  "choices": [
    {
      "logprobs": null,
      "finish_reason": "stop",
      "native_finish_reason": "STOP",
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "```json\n{\n  \"type\": \"statement\",\n  \"message\": \"Hello! Ready to create something amazing in Blender today?\"\n}\n```",
        "refusal": null,
        "reasoning": "Thinking... \n\n**Analyzing the Query's Nature**\n\nThe user's \"hello\" is a simple greeting, as noted in their query. It's not a request for information or action, which I must consider.\n\n \n \n\n\n**Processing User Input**\n\nThe user said \"hello\". Instructions state to categorize this as a greeting. Then a friendly statement is appropriate, I will maintain the query type.\n\n\n"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 2563,
    "completion_tokens": 115,
    "total_tokens": 2678
  }
}
Attempting to parse LLM response JSON
Direct JSON parsing succeeded
--- /llm-agent endpoint hit ---
--- Raw OpenRouter API Response ---
{
  "id": "gen-1747165615-KbyiTxIt5HLm2XFz2Qlc",
  "provider": "Google",
  "model": "google/gemini-2.5-flash-preview:thinking",
  "object": "chat.completion",
  "created": 1747165615,
  "choices": [
    {
      "logprobs": null,
      "finish_reason": "stop",
      "native_finish_reason": "STOP",
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "{\n  \"type\": \"statement\",\n  \"message\": \"Hello! Ready to create something amazing in Blender today?\"\n}",
        "refusal": null,
        "reasoning": "**Analyzing Initial Input**\n\nOkay, a simple \"hello\" from the user. Just as I thought, this isn't directly related to Blender functionality. I'll respond quickly, with excitement, and steer it back towards the core topic: Blender.\n\n\n\n"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 2563,
    "completion_tokens": 77,
    "total_tokens": 2640
  }
}
Attempting to parse LLM response JSON
Direct JSON parsing succeeded
